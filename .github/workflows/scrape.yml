name: Daily Scraper (Auto)

on:
  workflow_dispatch:

  schedule:
  
    # 1) AEDT (UTC+11) — Daylight Saving Months
    #    Covers October–March.
    #    08:00 AEDT = 21:00 UTC (previous day)
    #
    #    This cron triggers at 21:00 UTC, which becomes 08:00 AEDT.
    - cron: '0 21 * 10-12,1-3 *'
  
    # 2) AEST (UTC+10) — Non-DST Months
    #    Covers April–September.
    #    08:00 AEST = 22:00 UTC (previous day)
    #
    #    This cron triggers at 22:00 UTC, which becomes 08:00 AEST.
    - cron: '0 22 * 4-9 *'
  #
  # NOTE:
  # - GitHub Actions cron is ALWAYS UTC.
  # - Australia switches DST on two specific Sundays.
  #   Cron cannot target exact Sundays, so month-range scheduling is the only practical solution.
  # - This setup guarantees:
  #     → 08:00 AEST during AEST months
  #     → 08:00 AEDT during AEDT months
  #
  # This is the standard, reliable way to handle Australian time zones in GitHub Actions.


jobs:
  scrape:
    runs-on: ubuntu-latest
    
    env:
      CI: 'true'  # Set CI flag so scraper uses same-day pickup date
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libnss3 libnspr4 libatk1.0-0t64 libatk-bridge2.0-0t64 libcups2t64 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libasound2t64
      
      - name: Install Python dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip3 install -r requirements.txt
      
      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-chromium-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-playwright-chromium-
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium
      
      - name: Load environment variables
        run: |
          echo "SUPABASE_DB_HOST=${{ secrets.SUPABASE_DB_HOST }}" >> $GITHUB_ENV
          echo "SUPABASE_DB_PORT=${{ secrets.SUPABASE_DB_PORT }}" >> $GITHUB_ENV
          echo "SUPABASE_DB_NAME=${{ secrets.SUPABASE_DB_NAME }}" >> $GITHUB_ENV
          echo "SUPABASE_DB_USER=${{ secrets.SUPABASE_DB_USER }}" >> $GITHUB_ENV
          echo "SUPABASE_DB_PASSWORD=${{ secrets.SUPABASE_DB_PASSWORD }}" >> $GITHUB_ENV
          echo "R2_ACCOUNT_ID=${{ secrets.R2_ACCOUNT_ID }}" >> $GITHUB_ENV
          echo "R2_ACCESS_KEY_ID=${{ secrets.R2_ACCESS_KEY_ID }}" >> $GITHUB_ENV
          echo "R2_SECRET_ACCESS_KEY=${{ secrets.R2_SECRET_ACCESS_KEY }}" >> $GITHUB_ENV
          echo "R2_BUCKET_NAME=${{ secrets.R2_BUCKET_NAME }}" >> $GITHUB_ENV
          echo "R2_PUBLIC_URL=${{ secrets.R2_PUBLIC_URL }}" >> $GITHUB_ENV
      
      - name: Run scraper
        run: |
          python3 scrape.py
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            scraper.log
            *.log
          retention-days: 7
