name: Daily Scraper (Auto)

on:
  workflow_dispatch:
  # schedule:
  #   # Run at 8:00 AM AEST every day
  #   # AEST is UTC+10, so 8:00 AM AEST = 22:00 UTC (previous day)
  #   # AEDT is UTC+11, so 8:00 AM AEDT = 21:00 UTC (previous day)
  #   # Using 22:00 UTC to cover both AEST and AEDT (closer to AEST)
  #   # Note: GitHub Actions uses UTC timezone
  #   - cron: '0 22 * * *'  # 8:00 AM AEST (22:00 UTC previous day)

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    env:
      CI: 'true'  # Set CI flag so scraper uses same-day pickup date
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libnss3 libnspr4 libatk1.0-0t64 libatk-bridge2.0-0t64 libcups2t64 libdrm2 libxkbcommon0 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libasound2t64
      
      - name: Install Python dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip3 install -r requirements.txt
      
      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-chromium-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-playwright-chromium-
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium
      
      - name: Load environment variables
        run: |
          echo "SUPABASE_DB_HOST=${{ secrets.SUPABASE_DB_HOST }}" >> $GITHUB_ENV
          echo "SUPABASE_DB_PORT=${{ secrets.SUPABASE_DB_PORT }}" >> $GITHUB_ENV
          echo "SUPABASE_DB_NAME=${{ secrets.SUPABASE_DB_NAME }}" >> $GITHUB_ENV
          echo "SUPABASE_DB_USER=${{ secrets.SUPABASE_DB_USER }}" >> $GITHUB_ENV
          echo "SUPABASE_DB_PASSWORD=${{ secrets.SUPABASE_DB_PASSWORD }}" >> $GITHUB_ENV
          echo "R2_ACCOUNT_ID=${{ secrets.R2_ACCOUNT_ID }}" >> $GITHUB_ENV
          echo "R2_ACCESS_KEY_ID=${{ secrets.R2_ACCESS_KEY_ID }}" >> $GITHUB_ENV
          echo "R2_SECRET_ACCESS_KEY=${{ secrets.R2_SECRET_ACCESS_KEY }}" >> $GITHUB_ENV
          echo "R2_BUCKET_NAME=${{ secrets.R2_BUCKET_NAME }}" >> $GITHUB_ENV
          echo "R2_PUBLIC_URL=${{ secrets.R2_PUBLIC_URL }}" >> $GITHUB_ENV
      
      - name: Run scraper
        run: |
          python3 scrape.py
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            scraper.log
            *.log
          retention-days: 7
